results <- na.omit(results)
MSE.lm.age<- mse(predicted = results$predictions.lm.age, actual = results$greensand.legTime)
MSE.lm.age
predictions.lm.age<-predict(lm.fit.interaction2,
newdata = greensand21[,c("year","gender","age","leg","legLen")])
results <- data.frame(predictions.lm.age, greensand21$legTime)
results <- na.omit(results)
head(results)
MSE.lm.age<- mse(predicted = results$predictions.lm.age, actual = results$greensand.legTime)
MSE.lm.age
MSE.lm.age<- mse(predicted = results$predictions.lm.age, actual = results$greensand21.legTime)
MSE.lm.age
MAPE.lm.age<-mape(predicted = results$predictions.lm.age, actual = results$greensand21.legTime)
MAPE.lm.age
load(file = file.choose())
dim(pwt)
head(pwt)
pwt$log.gdp<-log(pwt$gdp)
pwt$log.consumption<-log(pwt$consumption)
head(pwt)
plot(pwt$log.gdp, pwt$log.consumption, xlab = "Log GDP", ylab = "Log Consumption",
main = "Scatter plot of Log Consumption and Log GDP")
model <- lm(log.consumption ~ log.gdp, data = data)
model <- lm(log.consumption ~ log.gdp, data = pwt)
intercept <- coef(model)[1]
slope <- coef(model)[2]
intercept
slope
# Convert countryCode to a factor variable
pwt$countryCode <- as.factor(data$countryCode)
# Convert countryCode to a factor variable
pwt$countryCode <- as.factor(pwt$countryCode)
head(pwt)
# Estimate the LSDV regression model
LSDV.model <- plm(log.consumption ~ log.gdp, data = pwt, index = "countryCode", model = "within")
# Load the necessary libraries
library(plm)
install.packages("plm")
# Estimate the LSDV regression model
LSDV.model <- plm(log.consumption ~ log.gdp, data = pwt, index = "countryCode", model = "within")
# Load the necessary libraries
library(plm)
# Load the necessary libraries
library("plm")
install.packages("plm")
install.packages("plm")
# Load the necessary libraries
library("plm")
# Estimate the LSDV regression model
LSDV.model <- plm(log.consumption ~ log.gdp, data = pwt, index = "countryCode", model = "within")
# Print the regression results
summary(model)
summary(model)
summary(LSDV.model)
random <- plm(log.consumption ~ log.gdp, data=Panel, index= "contryCode", model="random")
random <- plm(log.consumption ~ log.gdp, data = pwt, index= "contryCode", model="random")
random <- plm(log.consumption ~ log.gdp, data = pwt, index= "countryCode", model="random")
summary(random)
load(file = file.choose())
conBhutan<-pwt[pwt$country=="Bhutan","consumption"]
gdpBhutan<-pwt[pwt$country=="Bhutan","gdp"]
lconBhutan<-log(conBhutan)
lgdpBhutan<-log(gdpBhutan)
lconBhutanTS<-ts(exp(lconBhutan), start = c(2023, 1), frequency = 12)
lconBhutanTS
plot(lconBhutanTS, xlab = "Year", ylab = "consumption", main = "Time Series Plot")
lconBhutanTS<-ts(lconBhutan, start = c(2023, 1), frequency = 12)
plot(lconBhutanTS, xlab = "Year", ylab = "consumption", main = "Time Series Plot")
lgdpBhutanTS<-ts(lgdpBhutan, start = c(2023, 1), frequency = 12)
plot(lgdpBhutanTS, xlab = "Year", ylab = "log(GDP)", main = "Time Series Plot")
par(mfrow = c(2, 1))  # Divide the plot area into 2 rows and 1 column
# Plot time series 1
plot(lconBhutanTS, xlab = "Year", ylab = "log(consumption)", main = "Time Series Plot")
# Plot time series 2
plot(lgdpBhutanTS, xlab = "Year", ylab = "log(GDP)", main = "Time Series Plot")
# Calculate and plot first differences of lconBhutanTS and lgdpBhutanTS
first_diff1 <- diff(lconBhutanTS)
first_diff2 <- diff(lgdpBhutanTS)
# Plot first differences
par(mfrow = c(2, 1))  # Divide the plot area into 2 rows and 1 column
# Plot first differences of time series lconBhutanTS
plot(first_diff1, xlab = "Year", ylab = "First Difference", main = "First Differences of lconBhutanTS")
# Plot first differences of lgdpBhutanTS
plot(first_diff2, xlab = "Year", ylab = "First Difference", main = "First Differences of lgdpBhutanTS")
#cointegration test
# Load required libraries
library(tseries)
#cointegration test
# Load required libraries
install.packages("tseries")
library(tseries)
# Test for cointegration
coint_test <- ca.jo(data.frame(lconBhutanTS, lgdpBhutanTS), K = 1, type = "trace")
library(tseries)
library("urca")
install.packages("urca")
library("urca")
# Test for cointegration
coint_test <- (data.frame(lconBhutanTS, lgdpBhutanTS), K = 1, type = "trace")
# Test for cointegration
coint_test <- ca.jo(data.frame(lconBhutanTS, lgdpBhutanTS), K = 1, type = "trace")
# Test for cointegration
coint_test <- ca.jo(data.frame(lconBhutanTS, lgdpBhutanTS), K = 2, type = "trace")
summary(coint_test)
# Create joint time plot
plot(cbind(lconBhutanTS, lgdpBhutanTS), xlab = "Time", ylab = "Value",
main = "Joint Time Plot of Cointegrated Series")
legend("topleft", legend = c("lconBhutanTS", "lgdpBhutanTS"), col = 1:2, lty = 1)
# Create joint time plot
plot(cbind(lconBhutanTS, lgdpBhutanTS), xlab = "Time", ylab = "Value",
main = "Joint Time Plot of Cointegrated Series")
# Add cointegration line
abline(h = coef(coint_test)[1, 1], col = "red", lty = 2)
# Perform cointegration regression
coint_model <- ca.jo(data.frame(lconBhutanTS, lgdpBhutanTS), type = "trace", K = 1)
# Perform cointegration regression
coint_model <- ca.jo(data.frame(lconBhutanTS, lgdpBhutanTS), type = "trace", K = 2)
# Extract cointegration residuals
residuals <- coint_model$residuals
# Perform cointegration regression
coint_model <- ca.jo(data.frame(lconBhutanTS, lgdpBhutanTS), type = "trace", K = 2)
# Extract cointegration residuals
residuals <- residuals(coint_model)
# Extract cointegration residuals
residuals <- residuals(coint_model)
# Extract cointegration residuals
residuals <- residuals(coint_model)
library(urca)
# Perform cointegration regression
coint_model <- ca.jo(data.frame(lconBhutanTS, lgdpBhutanTS), type = "trace", K = 2)
# Extract cointegration residuals
residuals <- residuals(coint_model)
regression.model<-lm(lconBhutanTS~lgdpBhutanTS)
residuals <- regression.model$residuals
# Convert residuals to time series
residuals_ts <- ts(residuals, start = c(1, 1))
# Plot the residuals as a time series
plot(residuals_ts, xlab = "Time", ylab = "Residuals", main = "Cointegration Residuals")
# Plot the ACF of the residuals
acf(residuals_ts, main = "ACF of Cointegration Residuals")
# Test for white noise using Ljung-Box test
ljung_box_test <- Box.test(residuals, lag = 10, type = "Ljung-Box")
# Print the Ljung-Box test results
cat("Ljung-Box test p-value:", ljung_box_test$p.value, "\n")
# Perform ADF test with lag selection using AIC
adf_test <- ur.df(data, type = "drift", selectlags = "AIC")
library(stringr)
library(tidyverse)
comments.data01<-read_csv(file = "Bangla Comments.csv", locale = locale(encoding = "UTF-8"))
View(comments.data01)
comments.data01<-comments.data01[,c("FinalTag","Comments")]
comments.data01<-comments.data01[comments.data01$FinalTag %in% c("Happiness","Sadness"),]
dim(comments.data01)
table(comments.data01$FinalTag)
index<-sample(1:length(comments.data01$FinalTag))
comments.data01<-comments.data01[index,]
table(comments.data01$FinalTag)
Labels<-comments.data01$FinalTag
Labels01<-ifelse(Labels=="Happiness",1,0)
Labels[1:10]
Labels01[1:10]
bangla.text <- str_remove_all(comments.data01$Comments, "[0-9]")
bangla.text <- str_remove_all(bangla.text, "[[:punct:]]")
bangla.text <- str_replace_all(bangla.text, "\\s+", " ") #Remove extra white spaces
bangla.tokens <- str_split(bangla.text, "\\s+")
class(bangla.tokens)
length(bangla.tokens)
save(bangla.tokens, file = "Bangla tokens happy sad.RData")
library(textstem)
text <- bangla.tokens[[3]]
lemmatized_text <- lemmatize_words(text, language = "bn")
lemmatized_text
bangla.lemmatized<-lemmatize_words(bangla.tokens, language= "bn")
class(bangla.lemmatized)
bangla.lemmatized[[3]]
save(bangla.lemmatized, file = "Bangla Lemmatized happy sad.RData")
library(text)
library(tm)
tokenized_text <- bangla.lemmatized # Your tokenized Bangla text
corpus <- Corpus(VectorSource(tokenized_text)) #Create a corpus from tokenized Bangla text
dtm <- DocumentTermMatrix(corpus, control = list(weighting = weightTfIdf))
tfidf_data <- as.data.frame(as.matrix(dtm))
class(tfidf_data)
save(tfidf_data, file = "Bangla text happy sad tf idf.RData")
dim(tfidf_data)
tfidf_data[1:10,1:10]
tfidf_data[1:10,1:8]
library(caret)
index<-createDataPartition(Labels, times = 1, p=0.7, list = FALSE)
index[1:10]
index[1:30]
train.data<-tfidf_data[index,]
test.data<-tfidf_data[-index,]
dim(train.data)
dim(test.data)
train.labels<-Labels[index]
length(train.labels)
test.labels<-Labels[-index]
length(test.labels)
library(e1071)
svm.model<-svm(train.data, train.labels)
train.labels<-Labels01[index]
length(train.labels)
test.labels<-Labels01[-index]
length(test.labels)
gc()
svm.model<-svm(train.data, train.labels)
save(Labels, file = "Bangla comments Label happy sad.RData")
save(Labels01, file = "Bangla comments Label binary happy sad.RData")
library(stringr)
library(tidyverse)
load(file = "Bangla comments Label happy sad.RData")
load(file = "Bangla comments Label binary happy sad.RData")
load(file = "Bangla text happy sad tf idf.RData")
library(caret)
index<-createDataPartition(Labels, times = 1, p=0.7, list = FALSE)
index[1:30]
train.data<-tfidf_data[index,]
dim(train.data)
test.data<-tfidf_data[-index,]
dim(test.data)
rm(tfidf_data)
train.labels<-Labels01[index]
length(train.labels)
test.labels<-Labels01[-index]
length(test.labels)
gc()
gc()
gc()
svm.model<-svm(train.data, train.labels)
library(e1071)
svm.model<-svm(train.data, train.labels)
library(stringr)
library(tidyverse)
comments.data01<-read_csv(file = "Bangla Comments.csv", locale = locale(encoding = "UTF-8"))
View(comments.data01)
comments.data01<-comments.data01[,c("FinalTag","Comments")]
comments.data01<-comments.data01[comments.data01$FinalTag %in% c("Happiness","Sadness"),]
dim(comments.data01)
table(comments.data01$FinalTag)
index<-sample(1:length(comments.data01$FinalTag))
comments.data01<-comments.data01[index,]
Labels<-comments.data01$FinalTag
Labels01<-ifelse(Labels=="Happiness",1,0)
Labels[1:10]
Labels01[1:10]
save(Labels, file = "Bangla comments Label happy sad.RData")
save(Labels01, file = "Bangla comments Label binary happy sad.RData")
bangla.text <- str_remove_all(comments.data01$Comments, "[0-9]")
bangla.text <- str_remove_all(bangla.text, "[[:punct:]]")
bangla.text <- str_replace_all(bangla.text, "\\s+", " ") #Remove extra white spaces
bangla.tokens <- str_split(bangla.text, "\\s+")
length(bangla.tokens)
save(bangla.tokens, file = "Bangla tokens happy sad.RData")
library(textstem)
bangla.lemmatized<-lemmatize_words(bangla.tokens, language= "bn")
save(bangla.lemmatized, file = "Bangla Lemmatized happy sad.RData")
library(text)
library(tm)
tokenized_text <- bangla.lemmatized #tokenized Bangla text
corpus <- Corpus(VectorSource(tokenized_text)) #Create a corpus from tokenized Bangla text
class(corpus)
save(corpus, file = "Bangla corpus.RData")
dtm <- DocumentTermMatrix(corpus)
term_df <- colSums(dtm > 0) #Compute document frequency for each term
tdm <- TermDocumentMatrix(corpus)
term_df <- colSums(tdm > 0) #Compute document frequency for each term
#another way to document frequency matrix
library(quanteda)
tokens_dfm<-dfm(tokenized_text, tolower = F)
tokens_dfm<-dfm(corpus, tolower = F)
bangla_text <- "আমার সোনার বাংলা"
tokens_bangla <- tokens(bangla_text, language = "bn")
stemmed_tokens <- tokens_wordstem(tokens_bangla, language = "bn")
tokens_bangla <- tokens(bangla_text)
stemmed_tokens <- tokens_wordstem(tokens_bangla, language = "bn")
temmed_tokens <- wordStem(tokens_bangla, language = "bn")
bangla_text <- "আমার সোনার বাংলা"
tokens_bangla <- tokens(bangla_text)
tokens_bangla
tokens<-tokens(comments.data01$Comments, what = "word", remove_punct = T, remove_numbers = T,
remove_symbols = T, split_hyphens = T)
tokens_dfm<-dfm(tokens, tolower = F)
tokens_dfm<-dfm(tokens, tolower = F)
dim(tokens_dfm)
bangla_text <- comments.data01$Comments[3]
tokens_bangla <- tokens(bangla_text)
tokens_bangla
dtm_tfidf <- weightTfIdf(tokens_dfm)
TF.function<-function(r){
r/sum(r)
} #This will take the frequency of a particular word in a document and divide by the total
#word count in that document
IDF.function<-function(c){
size<-length(c) #Count the total number of documents
doc.count<-length(which(c>0)) # For a certain word, count the number of documents where it appeared
log10(size/doc.count)
}
tf.idf<-function(tf,idf){
tf*idf
}
tokens_matrix<-as.matrix(tokens_dfm)
dim(tokens_matrix)
token.tf<-apply(tokens_matrix,1,TF.function) # '1' has been used for row operation
token.tf[1:10,1:10]
gc()
token.idf<-apply(tokens_matrix,2,IDF.function)
rm(tokens_matrix)
gc()
gc()
library(stringr)
library(tidyverse)
comments.data01<-read_csv(file = "Bangla Comments.csv", locale = locale(encoding = "UTF-8"))
comments.data01<-comments.data01[,c("FinalTag","Comments")]
comments.data01<-comments.data01[comments.data01$FinalTag %in% c("Happiness","Sadness"),]
dim(comments.data01)
table(comments.data01$FinalTag)
index<-sample(1:length(comments.data01$FinalTag))
comments.data01<-comments.data01[index,]
Labels<-comments.data01$FinalTag
Labels01<-ifelse(Labels=="Happiness",1,0)
save(Labels, file = "Bangla comments Label happy sad.RData")
save(Labels01, file = "Bangla comments Label binary happy sad.RData")
#another way to document frequency matrix
library(quanteda)
bangla_text <- comments.data01$Comments[3]
tokens_bangla <- tokens(bangla_text)
tokens_bangla
tokens<-tokens(comments.data01$Comments, what = "word", remove_punct = T, remove_numbers = T,
remove_symbols = T, split_hyphens = T)
tokens[[3]]
tokens_dfm<-dfm(tokens, tolower = F)
dim(tokens_dfm)
tokens_matrix<-as.matrix(tokens_dfm)
dim(tokens_matrix)
TF.function<-function(r){
r/sum(r)
} #This will take the frequency of a particular word in a document and divide by the total
#word count in that document
IDF.function<-function(c){
size<-length(c) #Count the total number of documents
doc.count<-length(which(c>0)) # For a certain word, count the number of documents where it appeared
log10(size/doc.count)
}
tf.idf<-function(tf,idf){
tf*idf
}
token.tf<-apply(tokens_matrix,1,TF.function) # '1' has been used for row operation
token.tf[1:10,1:10]
gc()
token.idf<-apply(tokens_matrix,2,IDF.function)
rm(tokens_matrix)
gc()
token.tf_idf<-apply(token.tf,2,tf.idf,idf=token.idf)
rm(token.tf)
gc()
token.tf_idf[1:14,1:9]
token.tf_idf<-t(token.tf_idf)
incomplete.case.index<-which(!complete.cases(token.tf_idf))
dim(token.tf_idf)
token.tf_idf[incomplete.case.index,]<-rep(0.0,ncol(token.tf_idf))
sum(which(!complete.cases(token.tf_idf)))
dim(token.tf_idf)
save(token.tf_idf, file = "Bangla token tf idf quanteda.RData")
gc()
token.tf_idf.df<-cbind(Labels, data.frame(token.tf_idf))
dim(token.tf_idf.df)
token.tf_idf.df[1:10,1:10]
names(token.tf_idf.df)<-make.names(names = names(token.tf_idf.df))
token.tf_idf.df[1:10,1:10]
save(token.tf_idf.df, file = "bangla tf idf data frame with Labels.RData")
library(caret)
index<-createDataPartition(token.tf_idf.df$Labels, times = 1, p=0.6, list = F)
set.seed(32984, sample.kind = "Rounding")
index<-createDataPartition(token.tf_idf.df$Labels, times = 1, p=0.6, list = F)
train.data<-token.tf_idf.df[index,]
dim(train.data)
test.data<-token.tf_idf.df[-index,]
rm(token.tf_idf.df)
gc()
train.data$Labels<-as.factor(train.data$Labels)
test.data$Labels<-as.factor(test.data$Labels)
library(e1071)
View(index)
svm.fit<-svm(Labels ~ ., data = train.data, kernel = "linear", cost = 10, scale = FALSE)
library(stringr)
library(tidyverse)
load(file = "bangla tf idf data frame with Labels.RData")
library(caret)
index<-createDataPartition(token.tf_idf.df$Labels, times = 1, p=0.6, list = F)
train.data<-token.tf_idf.df[index,]
dim(train.data)
test.data<-token.tf_idf.df[-index,]
dim(test.data)
rm(token.tf_idf.df)
gc()
train.data$Labels<-as.factor(train.data$Labels)
test.data$Labels<-as.factor(test.data$Labels)
library(e1071)
svm.fit<-svm(Labels ~ ., data = train.data, kernel = "linear", cost = 10, scale = FALSE)
saveRDS(object = svm.fit, file = "bangla svm.rds")
gc()
predictions<-predict(svm.fit, test.data[,-1])
confusionMatrix(predictions, test_set$Labels)
confusionMatrix(predictions, test.data$Labels)
gc()
confusionMatrix(predictions, test_set$Labels, mode = "everything")
confusionMatrix(predictions, test.data$Labels, mode = "everything")
#keras model for happiness and sadness
library(keras)
library(tensorflow)
load(file = "bangla tf idf data frame with Labels.RData")
Labels01.encoded<-to_categorical(token.tf_idf.df$Labels)
Labels<-token.tf_idf.df$Labels
Labels01<-ifelse(Labels=="Happiness",1,0)
Labels01.encoded<-to_categorical(Labels01)
model <- keras_model_sequential() %>%
# Specify the input shape
layer_dense(units = 64, activation = "relu", input_shape = ncol(token.tf_idf.df[,-1])) %>%
# add a dense layer with 40 units
layer_dense(units = 32, activation = "relu") %>%
# add the classifier on top
layer_dense(units = 2, activation = "sigmoid")
model %>% compile(
optimizer="rmsprop",
loss="binary_crossentropy",
metrics= c("accuracy")
)
history <- model %>% fit(
token.tf_idf.df[,-1], Labels01.encoded,
epochs = 20,
batch_size = 32,
validation_split=0.3)
model <- keras_model_sequential() %>%
# Specify the input shape
layer_dense(units = 64, activation = "relu", input_shape = ncol(token.tf_idf.df[,-1])) %>%
# add a dense layer with 40 units
layer_dense(units = 32, activation = "relu") %>%
# add the classifier on top
layer_dense(units = 2, activation = "sigmoid")
model %>% compile(
optimizer="rmsprop",
loss="binary_crossentropy",
metrics= c("accuracy")
)
history <- model %>% fit(
token.tf_idf.df[,-1], Labels01.encoded,
epochs = 20,
batch_size = 32,
validation_split=0.3)
library(reticulate)
dim(token.tf_idf.df)
token.tf_idf.df[1:5,1:5]
feature.data<-token.tf_idf.df[,-1]
dim(feature.data)
feature.data[1:5,1:5]
dim(token.tf_idf.df)
feature.data
dim(feature.data)
dim(Labels01)
length(Labels01)
model <- keras_model_sequential %>%
layer_dense(units = 64, activation = "relu", input_shape = c(12461)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
model %>%
compile(loss = "binary_crossentropy",
optimizer = "adam",
metrics = c("accuracy"))
model %>%
fit(x = feature.data,
y = Labels01,
epochs = 10,
batch_size = 32)
dim(Labels01.encoded)
model <- keras_model_sequential %>%
layer_dense(units = 64, activation = "relu", input_shape = c(12461)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
model <- keras_model_sequential %>%
layer_dense(units = 64, activation = "relu", input_shape = c(12461)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
model %>%
fit(x = feature.data,
y = Labels01.encoded,
epochs = 10,
batch_size = 32)
missing_values <- is.na(feature.data)
gc()
missing_count <- colSums(missing_values)
missing_count
total_missing <- sum(is.na(feature.data), na.rm = TRUE)
total_missing
total_missing <- sum(is.na(feature.data))
total_missing
head(Labels01.encoded)
model <- keras_model_sequential %>%
layer_dense(units = 64, activation = "relu", input_shape = c(12461)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
model %>%
compile(loss = "binary_crossentropy",
optimizer = "adam",
metrics = c("accuracy"))
model %>%
fit(x = feature.data,
y = Labels01.encoded,
epochs = 10,
batch_size = 32)
setwd("D:/Research Paper/Research new/Text Analytics R Codes")
setwd("D:/Research Paper/Research new/Text Analytics R Codes/Text-Analytics-Research-Codes")
